{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrb20KGMtTFq",
        "outputId": "3b6fc800-ed78-4353-8538-811b602e7d46"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "MTwiK0-ArwrT",
        "outputId": "001e468b-e9d1-406f-e896-f5231bc01a29"
      },
      "outputs": [],
      "source": [
        "# 1️⃣ تطبيع الصور (Normalization)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# 2️⃣ تحويل التصنيفات إلى One-Hot Encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 3️⃣ عرض 10 صور عشوائية لكل فئة من فئات CIFAR-10\n",
        "fig, axes = plt.subplots(10, 10, figsize=(12, 12))\n",
        "class_indices = {i: np.where(y_train.argmax(axis=1) == i)[0] for i in range(10)}\n",
        "\n",
        "for i in range(10):  # لكل فئة من 0 إلى 9\n",
        "    samples = np.random.choice(class_indices[i], 10, replace=False)  # اختر 10 صور عشوائية\n",
        "    for j, img_idx in enumerate(samples):\n",
        "        axes[i, j].imshow(x_train[img_idx])  # عرض الصورة\n",
        "        axes[i, j].axis('off')  # إخفاء المحاور\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "iSN6riPISBMG",
        "outputId": "a1c1cf7f-6301-45d1-d4eb-5d8c5d64e9d0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# إنشاء نموذج تسلسلي\n",
        "model = Sequential([\n",
        "    # 1️⃣ طبقة Convolutional (مرشح 3x3، عدد الفلاتر 32)\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "\n",
        "    # 2️⃣ طبقة Max Pooling (تصغير الحجم)\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "\n",
        "    # 3️⃣ Flatten لتحويل المخرجات إلى متجه يمكن إدخاله في Dense\n",
        "    Flatten(),\n",
        "\n",
        "    # 4️⃣ طبقة Fully Connected بـ 100 نيوترون\n",
        "    Dense(100, activation='relu'),\n",
        "\n",
        "    # 5️⃣ طبقة التصنيف النهائية (10 فئات، softmax)\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# طباعة ملخص النموذج\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn8UzPBZugVp",
        "outputId": "cc2acc0d-4c0d-4e43-ae4f-b65deef00807"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# تجميع النموذج\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n",
        "\n",
        "# تدريب النموذج\n",
        "history = model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcqyhg9UrwrY"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "oAiFHdFJrwrZ",
        "outputId": "d18913b2-c7dc-4ef7-c9b9-ceac97254418"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# استخراج بيانات التدريب من history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "# رسم منحنى الخسارة\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Cross Entropy Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "# رسم منحنى الدقة\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracy, 'bo-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r*-', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "cgca5dUNSFNc",
        "outputId": "0680596e-31b3-4021-fd21-5c39496b041f"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# تنظيف الجلسة\n",
        "from keras.backend import clear_session\n",
        "clear_session()\n",
        "\n",
        "# بناء النموذج\n",
        "model_vgg = Sequential()\n",
        "\n",
        "# إضافة الطبقات التلافيفية\n",
        "model_vgg.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model_vgg.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "# إضافة طبقة Max Pooling\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten لتحويل المصفوفة إلى متجه\n",
        "model_vgg.add(Flatten())\n",
        "\n",
        "# إضافة الطبقة Dense بـ 128 وحدة\n",
        "model_vgg.add(Dense(128, activation='relu'))\n",
        "\n",
        "# إضافة طبقة التصنيف النهائية (Softmax مع 10 وحدات)\n",
        "model_vgg.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# طباعة ملخص النموذج\n",
        "model_vgg.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "b9842468-c073-42a3-8c33-65af8bbe4496"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# تجميع النموذج\n",
        "model_vgg.compile(\n",
        "    loss='categorical_crossentropy',  # دالة فقدان التصنيف المتعدد\n",
        "    optimizer=SGD(learning_rate=0.01, momentum=0.9),  # استخدام learning_rate بدلاً من lr\n",
        "    metrics=['accuracy']  # قياس الدقة أثناء التدريب\n",
        ")\n",
        "\n",
        "# تدريب النموذج لمدة 50 حقبة باستخدام بيانات CIFAR-10\n",
        "history = model_vgg.fit(\n",
        "    x_train, y_train,  # بيانات التدريب\n",
        "    validation_data=(x_test, y_test),  # بيانات الاختبار للتحقق من الأداء\n",
        "    epochs=50,  # عدد الحقبات\n",
        "    batch_size=512,  # حجم الدفعة\n",
        "    verbose=1  # إظهار تقدم التدريب\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F8OSHAf5SJPr",
        "outputId": "131fa03f-ba00-422f-93f5-0405e384a3f4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# تنظيف الجلسة لضمان عدم وجود أي نماذج قديمة في الذاكرة\n",
        "from tensorflow.keras.backend import clear_session\n",
        "clear_session()\n",
        "\n",
        "# ✅ **1️⃣ بناء النموذج البسيط**\n",
        "model_simple = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# تجميع النموذج\n",
        "model_simple.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "# ✅ **2️⃣ بناء نموذج VGG الأعمق**\n",
        "model_vgg = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# تجميع نموذج VGG\n",
        "model_vgg.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "# ✅ **3️⃣ تدريب النموذج البسيط**\n",
        "history_simple = model_simple.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "# ✅ **4️⃣ تدريب نموذج VGG**\n",
        "history_vgg = model_vgg.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "# ✅ **5️⃣ استخراج بيانات التدريب للمقارنة**\n",
        "loss_simple = history_simple.history['loss']\n",
        "val_loss_simple = history_simple.history['val_loss']\n",
        "acc_simple = history_simple.history['accuracy']\n",
        "val_acc_simple = history_simple.history['val_accuracy']\n",
        "\n",
        "loss_vgg = history_vgg.history['loss']\n",
        "val_loss_vgg = history_vgg.history['val_loss']\n",
        "acc_vgg = history_vgg.history['accuracy']\n",
        "val_acc_vgg = history_vgg.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, 51)  # عدد الحقبات 50\n",
        "\n",
        "# ✅ **6️⃣ رسم منحنى الخسارة (Loss)**\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss_simple, 'r', label='Loss - Simple Model')\n",
        "plt.plot(epochs, val_loss_simple, 'r--', label='Val Loss - Simple Model')\n",
        "plt.plot(epochs, loss_vgg, 'b', label='Loss - VGG Model')\n",
        "plt.plot(epochs, val_loss_vgg, 'b--', label='Val Loss - VGG Model')\n",
        "plt.title('Comparison of Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# ✅ **7️⃣ رسم منحنى الدقة (Accuracy)**\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc_simple, 'r', label='Accuracy - Simple Model')\n",
        "plt.plot(epochs, val_acc_simple, 'r--', label='Val Accuracy - Simple Model')\n",
        "plt.plot(epochs, acc_vgg, 'b', label='Accuracy - VGG Model')\n",
        "plt.plot(epochs, val_acc_vgg, 'b--', label='Val Accuracy - VGG Model')\n",
        "plt.title('Comparison of Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Comment on the observation**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DObaoxhaSMUg",
        "outputId": "c469f73b-8d06-48a8-caa0-c80224040836"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ✅ **1️⃣ التنبؤ بالتصنيفات لبيانات الاختبار**\n",
        "y_pred = model_vgg.predict(x_test)  # استخراج الاحتمالات\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # استخراج الفئة ذات أعلى احتمال\n",
        "y_true = np.argmax(y_test, axis=1)  # الفئات الحقيقية\n",
        "\n",
        "# ✅ **2️⃣ حساب مصفوفة التشويش**\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# ✅ **3️⃣ رسم مصفوفة التشويش**\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - VGG Model')\n",
        "plt.show()\n",
        "\n",
        "# ✅ **4️⃣ تقرير الأداء لكل فئة**\n",
        "report = classification_report(y_true, y_pred_classes)\n",
        "print(\"Classification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "*(Double-click or enter to edit)*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "cd2f9c3c-abf7-411e-d8b5-ccd512eb9c6b"
      },
      "outputs": [],
      "source": [
        "# تقييم النموذج على بيانات الاختبار\n",
        "test_loss, test_acc = model_vgg.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "# طباعة الدقة\n",
        "print(f\"🔹 Test Accuracy: {test_acc * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "1ad88907-1fce-4d23-ebf7-01f7b00313f2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "# تنظيف الجلسة من أي نماذج سابقة\n",
        "clear_session()\n",
        "\n",
        "# بناء نموذج VGG\n",
        "model_vgg = Sequential()\n",
        "\n",
        "# ✅ 1️⃣ طبقتان Convolutional بـ 64 مرشحًا\n",
        "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
        "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))  # MaxPooling بعد الطبقات التلافيفية\n",
        "\n",
        "# ✅ 2️⃣ طبقتان Convolutional بـ 128 مرشحًا\n",
        "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))  # MaxPooling بعد الطبقات التلافيفية\n",
        "\n",
        "# ✅ 3️⃣ طبقتان Convolutional بـ 256 مرشحًا\n",
        "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))  # MaxPooling بعد الطبقات التلافيفية\n",
        "\n",
        "# ✅ 4️⃣ Flatten لتحويل المخرجات إلى شكل 1D\n",
        "model_vgg.add(Flatten())\n",
        "\n",
        "# ✅ 5️⃣ طبقة Dense بـ 128 وحدة\n",
        "model_vgg.add(Dense(128, activation='relu'))\n",
        "\n",
        "# ✅ 6️⃣ طبقة التصنيف النهائية (10 فئات - softmax)\n",
        "model_vgg.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# ✅ طباعة ملخص النموذج\n",
        "model_vgg.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4elnDWnjEbmO",
        "outputId": "7687a3b7-52e5-4b9e-e0c2-c3a9bd62280c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# ✅ تنظيف الجلسة\n",
        "clear_session()\n",
        "\n",
        "# ✅ **1️⃣ تحميل بيانات CIFAR-10 وإعادة تشكيل الصور إلى 64×64**\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# تحويل القيم إلى أرقام عشرية بين 0 و 1 (تطبيع البيانات)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# إعادة تحجيم الصور إلى 64×64\n",
        "x_train = tf.image.resize(x_train, (64, 64))\n",
        "x_test = tf.image.resize(x_test, (64, 64))\n",
        "\n",
        "# تحويل التصنيفات إلى One-Hot Encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# ✅ **2️⃣ بناء نموذج VGG**\n",
        "model_vgg = Sequential()\n",
        "\n",
        "# طبقتان Convolutional بـ 64 مرشحًا\n",
        "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
        "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# طبقتان Convolutional بـ 128 مرشحًا\n",
        "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# طبقتان Convolutional بـ 256 مرشحًا\n",
        "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "model_vgg.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# تحويل المخرجات إلى شكل 1D\n",
        "model_vgg.add(Flatten())\n",
        "\n",
        "# طبقة Dense بـ 128 وحدة\n",
        "model_vgg.add(Dense(128, activation='relu'))\n",
        "\n",
        "# طبقة التصنيف النهائية (10 فئات)\n",
        "model_vgg.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# ✅ **3️⃣ تجميع النموذج**\n",
        "model_vgg.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=SGD(learning_rate=0.01, momentum=0.9),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ✅ **4️⃣ تدريب النموذج لمدة 10 حقبات**\n",
        "history = model_vgg.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ✅ **5️⃣ التنبؤ بنتائج الاختبار**\n",
        "y_pred = model_vgg.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# ✅ **6️⃣ حساب مصفوفة التشويش**\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# ✅ **7️⃣ رسم مصفوفة التشويش**\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - VGG Model')\n",
        "plt.show()\n",
        "\n",
        "# ✅ **8️⃣ طباعة تقرير الأداء**\n",
        "report = classification_report(y_true, y_pred_classes)\n",
        "print(\"🔹 Classification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "_\n",
        "1 - Use of activation functions:\n",
        " Activation functions introduce non-linearity, allowing neural networks to learn complex patterns.\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "Sigmoid outputs independent probabilities (used for binary classification), while Softmax normalizes outputs into a probability distribution (used for multi-class classification).\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        "Binary crossentropy is for two-class problems, while categorical crossentropy is for multi-class classification with one-hot encoded labels."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
